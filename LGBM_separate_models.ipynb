{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPD0eraH7ORZonbuAjd0iWr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dots13/ForecastingStickerSalesKaggle/blob/main/LGBM_separate_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36mJGgm6WWe7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import holidays\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from scipy.fftpack import fft\n",
        "\n",
        "import sklearn\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "_BTKoi4QWix-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['date'] = pd.to_datetime(train['date'])\n",
        "\n",
        "# Filter rows with dates after 2015\n",
        "train = train[train['date'].dt.year > 2015]"
      ],
      "metadata": {
        "id": "8qGOiP5Kbmd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gdp_per_capita(alpha3, year):\n",
        "    \"\"\"\n",
        "    Fetch GDP per capita for a specific country and year from the World Bank API.\n",
        "\n",
        "    \"\"\"\n",
        "    url = f'https://api.worldbank.org/v2/country/{alpha3}/indicator/NY.GDP.PCAP.CD?date={year}&format=json'\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        return data[1][0]['value'] if data[1] else None\n",
        "    except (requests.RequestException, KeyError, IndexError) as e:\n",
        "        print(f\"Error fetching data for {alpha3} in {year}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "bbKBuo0gWvVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gdp_dataframe(alpha3s, years, country_names):\n",
        "    \"\"\"\n",
        "    Create a DataFrame of normalized GDP per capita ratios for multiple countries and years.\n",
        "\n",
        "    \"\"\"\n",
        "    # Fetch GDP data for all countries and years\n",
        "    gdp_data = [\n",
        "        [get_gdp_per_capita(alpha3, year) for year in years]\n",
        "        for alpha3 in alpha3s\n",
        "    ]\n",
        "\n",
        "    # Create a DataFrame with countries as rows and years as columns\n",
        "    gdp_df = pd.DataFrame(gdp_data, index=country_names, columns=years)\n",
        "\n",
        "    # Normalize GDP values by dividing by the column sum (yearly total)\n",
        "    gdp_df = gdp_df / gdp_df.sum(axis=0)\n",
        "\n",
        "    # Reshape the DataFrame into long format\n",
        "    gdp_df = gdp_df.reset_index().rename(columns={'index': 'country'})\n",
        "    gdp_df = gdp_df.melt(id_vars=['country'], var_name='year', value_name='ratio')\n",
        "\n",
        "    return gdp_df"
      ],
      "metadata": {
        "id": "HCkWtdThW3fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_ratios(gdp_df, adjustments):\n",
        "    \"\"\"\n",
        "    Adjust GDP ratios for specific countries based on custom rules.\n",
        "\n",
        "    Parameters:\n",
        "    - gdp_df: DataFrame containing GDP ratios.\n",
        "    - adjustments: Dictionary with country names as keys and adjustment values.\n",
        "\n",
        "    Returns:\n",
        "    - Adjusted DataFrame with updated ratios.\n",
        "    \"\"\"\n",
        "    adjusted_df = gdp_df.copy()\n",
        "\n",
        "    # Apply adjustments safely\n",
        "    for country, adjustment in adjustments.items():\n",
        "        if country in adjusted_df['country'].unique():\n",
        "            adjusted_df.loc[adjusted_df['country'] == country, 'ratio'] = (\n",
        "                adjusted_df.loc[adjusted_df['country'] == country, 'ratio'] - adjustment\n",
        "            ).clip(lower=0)  # Ensure ratios don't become negative\n",
        "\n",
        "    return adjusted_df"
      ],
      "metadata": {
        "id": "gBI-jsi0W5qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha3s = ['CAN', 'FIN', 'ITA', 'KEN', 'NOR', 'SGP']\n",
        "years = range(2010, 2020)\n",
        "country_names = np.array(['Canada', 'Finland', 'Italy', 'Kenya', 'Norway', 'Singapore'])  # Sorted automatically\n",
        "gdp_ratios_df = create_gdp_dataframe(alpha3s, years, country_names)\n",
        "adjustments = {'Kenya': 0.0007}\n",
        "gdp_per_capita_filtered_ratios_df = adjust_ratios(gdp_ratios_df, adjustments)\n",
        "print(gdp_per_capita_filtered_ratios_df.head(6))"
      ],
      "metadata": {
        "id": "WMX7CTHNW7W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_imputed = train.copy()\n",
        "print(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")\n",
        "\n",
        "# Extract the year from the date\n",
        "train_df_imputed['date'] = pd.to_datetime(train_df_imputed['date'])\n",
        "train_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\n",
        "\n",
        "# Loop through each year to perform imputation\n",
        "for year in train_df_imputed[\"year\"].unique():\n",
        "    # Target ratio (Norway)\n",
        "    target_ratio = gdp_per_capita_filtered_ratios_df.loc[\n",
        "        (gdp_per_capita_filtered_ratios_df[\"year\"] == year) &\n",
        "        (gdp_per_capita_filtered_ratios_df[\"country\"] == \"Norway\"), \"ratio\"\n",
        "    ].values[0]\n",
        "\n",
        "    # Impute Time Series 1: Canada, Discount Stickers, Holographic Goose\n",
        "    current_ratio_can = gdp_per_capita_filtered_ratios_df.loc[\n",
        "        (gdp_per_capita_filtered_ratios_df[\"year\"] == year) &\n",
        "        (gdp_per_capita_filtered_ratios_df[\"country\"] == \"Canada\"), \"ratio\"\n",
        "    ].values[0]\n",
        "    ratio_can = current_ratio_can / target_ratio\n",
        "    train_df_imputed.loc[\n",
        "        (train_df_imputed[\"country\"] == \"Canada\") &\n",
        "        (train_df_imputed[\"store\"] == \"Discount Stickers\") &\n",
        "        (train_df_imputed[\"product\"] == \"Holographic Goose\") &\n",
        "        (train_df_imputed[\"year\"] == year),\n",
        "        \"num_sold\"\n",
        "    ] = (\n",
        "        train_df_imputed.loc[\n",
        "            (train_df_imputed[\"country\"] == \"Norway\") &\n",
        "            (train_df_imputed[\"store\"] == \"Discount Stickers\") &\n",
        "            (train_df_imputed[\"product\"] == \"Holographic Goose\") &\n",
        "            (train_df_imputed[\"year\"] == year),\n",
        "            \"num_sold\"\n",
        "        ] * ratio_can\n",
        "    ).values\n",
        "\n",
        "    # Impute Time Series 2-3: Canada, Premium Sticker Mart / Stickers for Less\n",
        "    for store in [\"Premium Sticker Mart\", \"Stickers for Less\"]:\n",
        "        current_ts = train_df_imputed.loc[\n",
        "            (train_df_imputed[\"country\"] == \"Canada\") &\n",
        "            (train_df_imputed[\"store\"] == store) &\n",
        "            (train_df_imputed[\"product\"] == \"Holographic Goose\") &\n",
        "            (train_df_imputed[\"year\"] == year)\n",
        "        ]\n",
        "        missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
        "        train_df_imputed.loc[\n",
        "            (train_df_imputed[\"country\"] == \"Canada\") &\n",
        "            (train_df_imputed[\"store\"] == store) &\n",
        "            (train_df_imputed[\"product\"] == \"Holographic Goose\") &\n",
        "            (train_df_imputed[\"year\"] == year) &\n",
        "            (train_df_imputed[\"date\"].isin(missing_ts_dates)),\n",
        "            \"num_sold\"\n",
        "        ] = (\n",
        "            train_df_imputed.loc[\n",
        "                (train_df_imputed[\"country\"] == \"Norway\") &\n",
        "                (train_df_imputed[\"store\"] == store) &\n",
        "                (train_df_imputed[\"product\"] == \"Holographic Goose\") &\n",
        "                (train_df_imputed[\"year\"] == year) &\n",
        "                (train_df_imputed[\"date\"].isin(missing_ts_dates)),\n",
        "                \"num_sold\"\n",
        "            ] * ratio_can\n",
        "        ).values\n",
        "\n",
        "    # Impute Time Series 4: Kenya, Discount Stickers, Holographic Goose\n",
        "    current_ratio_ken = gdp_per_capita_filtered_ratios_df.loc[\n",
        "        (gdp_per_capita_filtered_ratios_df[\"year\"] == year) &\n",
        "        (gdp_per_capita_filtered_ratios_df[\"country\"] == \"Kenya\"), \"ratio\"\n",
        "    ].values[0]\n",
        "    ratio_ken = current_ratio_ken / target_ratio\n",
        "    train_df_imputed.loc[\n",
        "        (train_df_imputed[\"country\"] == \"Kenya\") &\n",
        "        (train_df_imputed[\"store\"] == \"Discount Stickers\") &\n",
        "        (train_df_imputed[\"product\"] == \"Holographic Goose\") &\n",
        "        (train_df_imputed[\"year\"] == year),\n",
        "        \"num_sold\"\n",
        "    ] = (\n",
        "        train_df_imputed.loc[\n",
        "            (train_df_imputed[\"country\"] == \"Norway\") &\n",
        "            (train_df_imputed[\"store\"] == \"Discount Stickers\") &\n",
        "            (train_df_imputed[\"product\"] == \"Holographic Goose\") &\n",
        "            (train_df_imputed[\"year\"] == year),\n",
        "            \"num_sold\"\n",
        "        ] * ratio_ken\n",
        "    ).values\n",
        "\n",
        "    # Impute Time Series 5-6: Kenya, Premium Sticker Mart / Stickers for Less\n",
        "    for store in [\"Premium Sticker Mart\", \"Stickers for Less\"]:\n",
        "        current_ts = train_df_imputed.loc[\n",
        "            (train_df_imputed[\"country\"] == \"Kenya\") &\n",
        "            (train_df_imputed[\"store\"] == store) &\n",
        "            (train_df_imputed[\"product\"] == \"Holographic Goose\") &\n",
        "            (train_df_imputed[\"year\"] == year)\n",
        "        ]\n",
        "        missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
        "        train_df_imputed.loc[\n",
        "            (train_df_imputed[\"country\"] == \"Kenya\") &\n",
        "            (train_df_imputed[\"store\"] == store) &\n",
        "            (train_df_imputed[\"product\"] == \"Holographic Goose\") &\n",
        "            (train_df_imputed[\"year\"] == year) &\n",
        "            (train_df_imputed[\"date\"].isin(missing_ts_dates)),\n",
        "            \"num_sold\"\n",
        "        ] = (\n",
        "            train_df_imputed.loc[\n",
        "                (train_df_imputed[\"country\"] == \"Norway\") &\n",
        "                (train_df_imputed[\"store\"] == store) &\n",
        "                (train_df_imputed[\"product\"] == \"Holographic Goose\") &\n",
        "                (train_df_imputed[\"year\"] == year) &\n",
        "                (train_df_imputed[\"date\"].isin(missing_ts_dates)),\n",
        "                \"num_sold\"\n",
        "            ] * ratio_ken\n",
        "        ).values\n",
        "\n",
        "    # Impute Time Series 7: Kenya, Discount Stickers, Kerneler\n",
        "    current_ts = train_df_imputed.loc[\n",
        "        (train_df_imputed[\"country\"] == \"Kenya\") &\n",
        "        (train_df_imputed[\"store\"] == \"Discount Stickers\") &\n",
        "        (train_df_imputed[\"product\"] == \"Kerneler\") &\n",
        "        (train_df_imputed[\"year\"] == year)\n",
        "    ]\n",
        "    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
        "    train_df_imputed.loc[\n",
        "        (train_df_imputed[\"country\"] == \"Kenya\") &\n",
        "        (train_df_imputed[\"store\"] == \"Discount Stickers\") &\n",
        "        (train_df_imputed[\"product\"] == \"Kerneler\") &\n",
        "        (train_df_imputed[\"year\"] == year) &\n",
        "        (train_df_imputed[\"date\"].isin(missing_ts_dates)),\n",
        "        \"num_sold\"\n",
        "    ] = (\n",
        "        train_df_imputed.loc[\n",
        "            (train_df_imputed[\"country\"] == \"Norway\") &\n",
        "            (train_df_imputed[\"store\"] == \"Discount Stickers\") &\n",
        "            (train_df_imputed[\"product\"] == \"Kerneler\") &\n",
        "            (train_df_imputed[\"year\"] == year) &\n",
        "            (train_df_imputed[\"date\"].isin(missing_ts_dates)),\n",
        "            \"num_sold\"\n",
        "        ] * ratio_ken\n",
        "    ).values\n",
        "\n",
        "# Check for remaining missing values\n",
        "print(f\"Missing values remaining after imputation: {train_df_imputed['num_sold'].isna().sum()}\")\n",
        "\n",
        "# Manual imputation for specific IDs\n",
        "train_df_imputed.loc[train_df_imputed[\"id\"] == 23719, \"num_sold\"] = 4\n",
        "train_df_imputed.loc[train_df_imputed[\"id\"] == 207003, \"num_sold\"] = 195\n",
        "\n",
        "# Final check for missing values\n",
        "print(f\"Final missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")"
      ],
      "metadata": {
        "id": "fPxAxn2gW9a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['cat'] = train['country'] + '_' + train['store'] + '_' + train['product']"
      ],
      "metadata": {
        "id": "Pun72oQbi2Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train_df_imputed.copy()\n",
        "train['cat'] = train['country'] + '_' + train['store'] + '_' + train['product']\n",
        "train[\"year\"] = train[\"date\"].dt.year\n",
        "train[\"month\"] = train[\"date\"].dt.month\n",
        "train[\"day_of_week\"] = train[\"date\"].dt.dayofweek\n",
        "train[\"day_of_year\"] = train[\"date\"].dt.dayofyear\n",
        "train[\"week_of_year\"] = train[\"date\"].dt.isocalendar().week\n",
        "train[\"is_weekend\"] = train[\"day_of_week\"].isin([5, 6]).astype(int)\n",
        "train[\"quarter\"] = train[\"date\"].dt.quarter\n",
        "\n",
        "# Create mapping\n",
        "store_mapping = {k: v for v, k in enumerate(train[\"store\"].unique())}\n",
        "product_mapping = {k: v for v, k in enumerate(train[\"product\"].unique())}\n",
        "\n",
        "#train[\"store_encoded\"] = train[\"store\"].map(store_mapping)\n",
        "#train[\"productt_encoded\"] = train[\"product\"].map(product_mapping)\n",
        "\n",
        "### **Adding Holiday Feature**\n",
        "country_holiday_map = {\n",
        "    \"Canada\": \"CA\",\n",
        "    \"Finland\": \"FI\",\n",
        "    \"Italy\": \"IT\",\n",
        "    \"Kenya\": \"KE\",\n",
        "    \"Norway\": \"NO\",\n",
        "    \"Singapore\": \"SG\"\n",
        "}\n",
        "\n",
        "# Create holiday dictionaries for each country\n",
        "holidays_dict = {\n",
        "    country: holidays.country_holidays(iso_code, years=range(train[\"year\"].min(), train[\"year\"].max() + 1))\n",
        "    for country, iso_code in country_holiday_map.items()\n",
        "}\n",
        "\n",
        "train[\"is_holiday\"] = train.apply(lambda row: int(row[\"date\"] in holidays_dict[row[\"country\"]]), axis=1)\n",
        "\n",
        "### **Adding Sin/Cos Features for Cyclical Encoding**\n",
        "train[\"month_sin\"] = np.sin(2 * np.pi * train[\"month\"] / 12.0)\n",
        "train[\"month_cos\"] = np.cos(2 * np.pi * train[\"month\"] / 12.0)\n",
        "train[\"day_of_week_sin\"] = np.sin(2 * np.pi * train[\"day_of_week\"] / 7.0)\n",
        "train[\"day_of_week_cos\"] = np.cos(2 * np.pi * train[\"day_of_week\"] / 7.0)\n",
        "train[\"day_of_year_sin\"] = np.sin(2 * np.pi * train[\"day_of_year\"] / 365.0)\n",
        "train[\"day_of_year_cos\"] = np.cos(2 * np.pi * train[\"day_of_year\"] / 365.0)\n",
        "train[\"week_of_year_sin\"] = np.sin(2 * np.pi * train[\"week_of_year\"] / 52.0)\n",
        "train[\"week_of_year_cos\"] = np.cos(2 * np.pi * train[\"week_of_year\"] / 52.0)\n",
        "\n",
        "train[\"half_year_sin\"] = np.sin(2 * np.pi * train[\"month\"] / 6)\n",
        "train[\"half_year_cos\"] = np.cos(2 * np.pi * train[\"month\"] / 6)\n",
        "\n",
        "# 2-year cycle\n",
        "train[\"two_year_sin\"] = np.sin(2 * np.pi * train[\"year\"] / 2)\n",
        "train[\"two_year_cos\"] = np.cos(2 * np.pi * train[\"year\"] / 2)\n",
        "\n",
        "# 3-year cycle\n",
        "train[\"three_year_sin\"] = np.sin(2 * np.pi * train[\"year\"] / 3)\n",
        "train[\"three_year_cos\"] = np.cos(2 * np.pi * train[\"year\"] / 3)\n",
        "\n",
        "# 4-year cycle\n",
        "train[\"four_year_sin\"] = np.sin(2 * np.pi * train[\"year\"] / 4)\n",
        "train[\"four_year_cos\"] = np.cos(2 * np.pi * train[\"year\"] / 4)\n",
        "\n",
        "## # Add country ratios"
      ],
      "metadata": {
        "id": "EtUO2pENXBVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "A8LvDnX-XSCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.columns"
      ],
      "metadata": {
        "id": "a62uXDxpXyaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = [\"year\",\n",
        "            \"month\",\n",
        "            \"day_of_week\",\n",
        "            \"day_of_year\",\n",
        "            \"week_of_year\",\n",
        "            \"is_weekend\",\n",
        "            \"quarter\",\n",
        "            #'store_encoded',\n",
        "            #'productt_encoded',\n",
        "            'is_holiday',\n",
        "            'month_sin',\n",
        "            'month_cos',\n",
        "            'day_of_week_sin',\n",
        "            'day_of_week_cos',\n",
        "            'day_of_year_sin',\n",
        "            'day_of_year_cos',\n",
        "            'week_of_year_sin',\n",
        "            'week_of_year_cos',\n",
        "            'half_year_sin',\n",
        "            'half_year_cos',\n",
        "            'two_year_sin',\n",
        "            'two_year_cos',\n",
        "            'three_year_sin',\n",
        "            'three_year_cos',\n",
        "            'four_year_sin',\n",
        "            'four_year_cos'\n",
        "            ]"
      ],
      "metadata": {
        "id": "4W7n5c0FZZSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdp_per_capita_filtered_ratios_df"
      ],
      "metadata": {
        "id": "FlHc6W-aaSJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_unique = train['cat'].unique()\n",
        "models = {}\n",
        "df = train.copy()\n",
        "\n",
        "for cu in cat_unique:\n",
        "    print(f\"Training model for {cu}...\")\n",
        "\n",
        "    # Filter data for the country\n",
        "    country_data = df[df['cat'] == cu]\n",
        "\n",
        "    X_train, y_train = country_data[features], country_data[\"num_sold\"]\n",
        "\n",
        "    # Define LightGBM parameters (you can tune these)\n",
        "    lgb_model = LGBMRegressor(\n",
        "    boosting_type=\"gbdt\",\n",
        "    objective=\"poisson\",\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=5,\n",
        "    random_state=0\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    lgb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Store the model\n",
        "    models[cu] = lgb_model\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred =lgb_model.predict(X_train)\n",
        "\n",
        "    # Calculate the error\n",
        "    mape = mean_absolute_percentage_error(y_train, y_pred)\n",
        "    print(f\"Mean Absolute Percentage Error (MAPE) for {cu}: {mape}\")"
      ],
      "metadata": {
        "id": "H8oyDSaSX2tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models"
      ],
      "metadata": {
        "id": "4Noxxdfxb6NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['date'] = pd.to_datetime(test['date'])\n",
        "test['cat'] = test['country'] + '_' + test['store'] + '_' + test['product']\n",
        "test[\"year\"] = test[\"date\"].dt.year\n",
        "test[\"month\"] = test[\"date\"].dt.month\n",
        "test[\"day_of_week\"] = test[\"date\"].dt.dayofweek\n",
        "test[\"day_of_year\"] = test[\"date\"].dt.dayofyear\n",
        "test[\"week_of_year\"] = test[\"date\"].dt.isocalendar().week\n",
        "test[\"is_weekend\"] = test[\"day_of_week\"].isin([5, 6]).astype(int)\n",
        "test[\"quarter\"] = test[\"date\"].dt.quarter\n",
        "\n",
        "# Create mapping\n",
        "store_mapping = {k: v for v, k in enumerate(test[\"store\"].unique())}\n",
        "product_mapping = {k: v for v, k in enumerate(test[\"product\"].unique())}\n",
        "\n",
        "### **Adding Holiday Feature**\n",
        "country_holiday_map = {\n",
        "    \"Canada\": \"CA\",\n",
        "    \"Finland\": \"FI\",\n",
        "    \"Italy\": \"IT\",\n",
        "    \"Kenya\": \"KE\",\n",
        "    \"Norway\": \"NO\",\n",
        "    \"Singapore\": \"SG\"\n",
        "}\n",
        "\n",
        "# Create holiday dictionaries for each country\n",
        "holidays_dict = {\n",
        "    country: holidays.country_holidays(iso_code, years=range(test[\"year\"].min(), test[\"year\"].max() + 1))\n",
        "    for country, iso_code in country_holiday_map.items()\n",
        "}\n",
        "\n",
        "test[\"is_holiday\"] = test.apply(lambda row: int(row[\"date\"] in holidays_dict[row[\"country\"]]), axis=1)\n",
        "\n",
        "### **Adding Sin/Cos Features for Cyclical Encoding**\n",
        "test[\"month_sin\"] = np.sin(2 * np.pi * test[\"month\"] / 12.0)\n",
        "test[\"month_cos\"] = np.cos(2 * np.pi * test[\"month\"] / 12.0)\n",
        "test[\"day_of_week_sin\"] = np.sin(2 * np.pi * test[\"day_of_week\"] / 7.0)\n",
        "test[\"day_of_week_cos\"] = np.cos(2 * np.pi * test[\"day_of_week\"] / 7.0)\n",
        "test[\"day_of_year_sin\"] = np.sin(2 * np.pi * test[\"day_of_year\"] / 365.0)\n",
        "test[\"day_of_year_cos\"] = np.cos(2 * np.pi * test[\"day_of_year\"] / 365.0)\n",
        "test[\"week_of_year_sin\"] = np.sin(2 * np.pi * test[\"week_of_year\"] / 52.0)\n",
        "test[\"week_of_year_cos\"] = np.cos(2 * np.pi * test[\"week_of_year\"] / 52.0)\n",
        "\n",
        "test[\"half_year_sin\"] = np.sin(2 * np.pi * test[\"month\"] / 6)\n",
        "test[\"half_year_cos\"] = np.cos(2 * np.pi * test[\"month\"] / 6)\n",
        "\n",
        "# 2-year cycle\n",
        "test[\"two_year_sin\"] = np.sin(2 * np.pi * test[\"year\"] / 2)\n",
        "test[\"two_year_cos\"] = np.cos(2 * np.pi * test[\"year\"] / 2)\n",
        "\n",
        "# 3-year cycle\n",
        "test[\"three_year_sin\"] = np.sin(2 * np.pi * test[\"year\"] / 3)\n",
        "test[\"three_year_cos\"] = np.cos(2 * np.pi * test[\"year\"] / 3)\n",
        "\n",
        "# 4-year cycle\n",
        "test[\"four_year_sin\"] = np.sin(2 * np.pi * test[\"year\"] / 4)\n",
        "test[\"four_year_cos\"] = np.cos(2 * np.pi * test[\"year\"] / 4)"
      ],
      "metadata": {
        "id": "s_CuwOQ6cheE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.columns"
      ],
      "metadata": {
        "id": "EK8eLw4Dc2LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = test.copy()\n",
        "predictions = {}\n",
        "\n",
        "final_predictions = []\n",
        "for cu in cat_unique:\n",
        "    print(f\"Making predictions for {cu}...\")\n",
        "    country_test_data = test_data[test_data['cat'] == cu]\n",
        "    X_test = country_test_data[features]\n",
        "    model = models[cu]\n",
        "    y_pred = model.predict(X_test)\n",
        "    country_test_data['predicted_num_sold'] = y_pred\n",
        "    country_predictions = country_test_data[['id', 'predicted_num_sold']]\n",
        "    final_predictions.append(country_predictions)\n",
        "\n",
        "    print(f\"Predictions for {cu} added to the final dataframe.\")\n",
        "\n",
        "all_predictions_df = pd.concat(final_predictions, ignore_index=True)\n",
        "print(all_predictions_df.head())"
      ],
      "metadata": {
        "id": "YDu8DbqTcUPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_predictions_df"
      ],
      "metadata": {
        "id": "Z8XpbrOYdatp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_predictions_df.columns = ['id', 'num_sold']"
      ],
      "metadata": {
        "id": "V5V0pXHVeFYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_predictions_df.set_index(\"id\", inplace=True)\n",
        "all_predictions_df[\"num_sold\"] = all_predictions_df[\"num_sold\"].round()\n",
        "all_predictions_df.to_csv(\"submission_13.csv\")"
      ],
      "metadata": {
        "id": "-YsDXtIadnae"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}